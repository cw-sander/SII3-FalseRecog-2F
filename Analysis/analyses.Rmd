---
title: "Analyses for SII Study 3"
output: 
  html_notebook:
  code_folding: hide
---

```{r setup, include=FALSE}
# R version 4.2.1
# load packages & set options
library(dplyr) # dplyr_1.0.10
library(ggdist) # ggdist_3.2.0
library(ggplot2) # ggplot2_3.3.6
library(ggpubr) # ggpubr_0.4.0
library(ggrepel, include.only = "geom_label_repel") # ggrepel_0.9.1
library(here) # here_1.0.1
library(lmerTest) # lmerTest_3.1-3
library(magrittr, include.only = "%T>%") # magrittr_2.0.3
library(rstatix) # rstatix_0.7.0
library(showtext) # showtext_0.9-5
library(tidyverse) # tidyverse_1.3.2

# Read data
d_long <- readRDS(here::here("Processed data/d-long.rds"))

# Demographics
dems <- d_long %>%
  select(subject_id, age, gender, starts_with("pol_")) %>%
  unique()

# Add custom font for plots
font_add("Nunito",
  regular = "/Users/carsten/Library/Fonts/NunitoSans-Regular.ttf",
  italic = "/Users/carsten/Library/Fonts/NunitoSans-Italic.ttf",
  bold = "/Users/carsten/Library/Fonts/NunitoSans-Bold.ttf",
  bolditalic = "/Users/carsten/Library/Fonts/NunitoSans-BoldItalic.ttf")
showtext_auto()

# Custom functions
formp <- function(p, text = FALSE) {
  ## ---------------------------
  ## Format p values
  ##
  ## This function takes in a number between
  ## zero and one or a formatted p-value and outputs
  ## a formatted p-value. If p-value is already formatted
  ## then applying the function changes the format from
  ## "p = .034" to ".034" and vice versa.
  ##
  ## @p p-value to be formatted
  ## @text adds "p = " or "p < " to output
  ##
  ## @out string with formatted p-value
  ## ---------------------------

  # If already formatted but no "p" then add "p"
  if (grepl("^<.\\d{3}$", p)) {
  out <- gsub("<", "p < ", p)
  } else if (grepl("^.\\d{3}$", p)) {
  out <- gsub("^", "p = ", p)
  # If already formatted and "p" then remove "p"
  } else if (grepl("^p < .\\d{3}$", p)) {
  out <- gsub("p < ", "<", p)
  } else if (grepl("^p = .\\d{3}$", p)) {
  out <- gsub("p = ", "", p)
  # If not yet formatted and smaller than .001
  } else if (is.numeric(p) && p < 0.001) {
  if (text) {
    out <- "p < .001"
  } else {
    out <- "<.001"
  }
  # If not yet formatted and bigger than .001
  } else if (p >= 0.001) {
  p <- format(round(p, 3), nsmall = 3, scientific = FALSE)
  p <- sub("0.", ".", p)
  if (text) {
    out <- paste("p =", p)
  } else {
    out <- p
  }
  }
  return(out)
}
forma <- function(number, dec = NULL, lead_zero = TRUE) { # nolint
  ## ---------------------------
  ## Format values in apa style
  ##
  ## This function takes in a number and outputs
  ## a formatted number. If no decimal is provided, then
  ## it uses a heuristic to round the number. If lead_zero
  ## is set to FALSE, then the lead zero of the number is
  ## removed (useful for p-values or eta squared).
  ##
  ## @number input number
  ## @dec optional number of decimals
  ## @lead_zero keep leading zero
  ##
  ## @out formatted number
  ## ---------------------------

  # If dec is logical, interpret as lead_zero
  if (is.logical(dec)) {
  lead_zero <- dec
  dec <- NULL
  }
  # If no decimal is specified, use heuristic
  if (!is.null(dec)) {
  } else if (abs(number) >= 100) {
  dec <- 0
  } else if (abs(number) >= 10 && number < 100) {
  dec <- 1
  } else if (abs(number) >= 0.1 && number < 10) {
  dec <- 2
  } else if (abs(number) >= 0.001 && number < 0.1) {
  dec <- 3
  } else if (abs(number) < 0.001 && number != 0) {
  dec <- stringr::str_locate(format(
    abs(number), scientific = FALSE), "[1-9]{1}")[1] - 2
  } else if (number == 0) {
  dec <- 0
  }
  # Round number to decimal
  out <- format(round(number, dec), nsmall = dec, scientific = FALSE)
  # Remove leading zero if required
  if (out < 1 && lead_zero == FALSE) {
  out <- sub("0.", ".", out)
  }
  return(out)
}
cor_table <- function(data, method = c("pearson", "spearman")) {
  # Compute correlation matrix
  pvalues <- data %>%
  cor_pmat(method = method[1]) %>%
  rowwise() %>%
  mutate(across(!1, formp))
  coefs <- data %>%
  cor_mat(method = method[1]) %>%
  rowwise() %>%
  mutate(across(!1, forma, 2))
  for (row in seq(2, nrow(coefs))) {
  for (col in seq(2, ncol(coefs) - 1)) {
    c <- coefs[row, col]
    p <- pvalues[row, col]
    coefs[row, col] <- paste0(c, " (", p, ")")
  }
  }
  coefs <- coefs %>%
  pull_lower_triangle() %>%
  slice(-1) %>%
  select(-last_col()) %>%
  rename(variable = 1)
  return(coefs)
}
mlm_compare <- function(new, old) {
  ## ---------------------------
  ## Compare two multilevel models
  ##
  ## Outputs a list with a test comparing the models ($test),
  ## a formatted p-value ($p), the variance explained by the
  ## random and fixed effects ($var_exp) and the percentage
  ## increase in variance explained in the new compared to
  ## the old model.
  ##
  ## @new new model
  ## @old old model to compare the new
  ##    model against
  ##
  ## @out list with statistics
  ## ---------------------------

  out <- list()

  # Model comparison
  out$test <- anova(new, old)
  out$p <- formp(out$test[2, "Pr(>Chisq)"], TRUE)

  # Stats new model
  var_new <- as.data.frame(lme4::VarCorr(new))
  var_exp <- data.frame(row.names = c("old", "new"))
  # Loop across random effects
  for (i in seq_along(var_new[, 1])) {
  var_exp_i <- var_new$vcov[i] / sum(var_new$vcov) * 100
  var_exp[2, var_new$grp[i]] <- var_exp_i
  }

  # Stats old model
  if (class(old) != "lm") {
  var_old <- as.data.frame(lme4::VarCorr(old))
  # Loop across random effects
  for (i in seq_along(var_old[, 1])) {
    var_exp_i <- var_old$vcov[i] / sum(var_old$vcov) * 100
    var_exp[1, var_old$grp[i]] <- var_exp_i
  }
  var_exp["diff", ] <- var_exp[1, ] - var_exp[2, ]
  var_exp_delta <- ((var_exp[3, ] / var_exp[1, ]) * 100)
  var_exp_delta <- sapply(var_exp_delta, forma)
  var_exp[] <- apply(var_exp, c(1, 2), forma)
  out$var_exp_delta <- var_exp_delta
  } else if (class(old) == "lm") {
  var_exp[2, ] <- sapply(var_exp[2, ], forma)
  }
  out$var_exp <- var_exp
  return(out)
}
theme_cs_talk <- function(font = "Nunito", lab_size = 16, label_size = 14,
  dark = "#465263", light = "#E1E9ED", solid_facet = TRUE) {
  if (solid_facet) {
  facet_fill <- dark
  facet_text <- light
  } else if (!solid_facet) {
  facet_fill <- "transparent"
  facet_text <- dark
  }
  theme_bw(base_size = 16) %+replace%
  theme(
  # Rectangle elements
  plot.background = element_rect(fill = "transparent",
    color = NA_character_),
  panel.background = element_rect(fill = "transparent"),
  legend.background = element_rect(fill = "transparent", color = NA),
  strip.background = element_rect(color = facet_fill,
    fill = facet_fill, size = 1),
  # Text elements
  plot.title = element_text(family = font, size = lab_size,
    face = "bold", hjust = 0, vjust = 2, color = dark),
  plot.subtitle = element_text(family = font,
    size = lab_size - 2, color = dark),
  plot.caption = element_text(family = font, size = lab_size,
    hjust = 1, color = dark),
  axis.title = element_text(family = font, size = lab_size,
    color = dark),
  axis.text = element_text(family = font, size = label_size,
    color = dark),
  axis.text.x = element_text(margin = margin(5, b = 10),
    color = dark),
  legend.title = element_text(family = font, size = lab_size,
    color = dark, hjust = 0),
  legend.text = element_text(family = font, size = label_size,
    color = dark),
  strip.text = element_text(family = font, size = label_size,
    color = facet_text, margin = margin(4, 4, 4, 4)),
  # Line elements
  axis.ticks = element_line(color = dark, size = 0.5),
  legend.key = element_rect(fill = "transparent", color = NA_character_),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.border = element_rect(color = dark, fill = NA, size = 1)
  )
}
```

### Introduction
Research on person perception has revealed that people tend to attribute other's behavior to stable person characteristics, even if it can just as well be explained by reasons such as situational factors or mental states. This tendency is known as the correspondence bias (Gilbert & Malone, 1995) and may play an important role in the domain of political polarization. Specifically, people may attribute each other's politically relevant behaviors to stable ideological dispositions (such as leftist, conservative, racist, or feminist), while neglecting potential other causes and thereby impeding mutual understanding. To investigate the role of the correspondence bias in political polarization, we examine whether spontaneous ideological inferences are reduced when behaviors are accompanied by information on relatively sufficient reasons for the behavior. We thus extend previous research on spontaneous trait inferences (STI, Winter & Uleman, 1984) and the correspondence bias to spontaneous inferences of ideological dispositions.

### Sample Characteristics
We collected data from a total of N = 414 participants. Following our pre-registered exclusion criteria, we excluded 1 participant who did not give informed consent, 0 who gave the same response in all of the 36 test trials, and 7 who rated their own data to be unfit for analysis. We further had to exclude 20 participants due to a programming error. This resulted in a sample of N = `r nrow(dems)` participants (`r nrow(filter(dems, gender == "female"))` female, `r nrow(filter(dems, gender == "male"))` male, `r nrow(filter(dems, gender == "other"))` other, `r nrow(filter(dems, gender %in% c("not specified","")))` not specified; average age M = `r round(mean(dems$age, na.rm = TRUE), 1)` years, SD = `r round(sd(dems$age, na.rm = TRUE), 1)`, ranging from `r min(dems$age, na.rm = TRUE)` to `r max(dems$age, na.rm = TRUE)`). Participants were recruited via the online platform Prolific (www.prolific.co) and received monetary compensation of 3.00 GBP for completing the 20-minute study. An additional 88 people started the experiment on prolific but either returned their submission, timed-out, or only partially completed the experiment due to technical issues.

On average, participants reported to be rather left leaning (M = `r round(mean(dems$pol_orientation, na.rm = TRUE), 1)`, SD = `r round(sd(dems$pol_orientation, na.rm = TRUE), 1)` on a scale from 1 = left to 10 = right) and rather interested in politics (M = `r round(mean(dems$pol_interest), 1)`, SD = `r round(sd(dems$pol_interest), 1)`, on a scale ranging from 1 = not at all to 10 = very strongly).

### Manipulation check
```{r manipulation check, message = FALSE}
check <- d_long %>%
  select(subject_id, reason_type, sufficiency) %>%
  group_by(subject_id, reason_type) %>%
  summarize(.groups = "drop", mn = mean(sufficiency, na.rm = TRUE))

# Descriptives
check_desc <- check %>%
  group_by(reason_type) %>%
  get_summary_stats(mn, type = "mean_sd") %>%
  rowwise() %>%
  mutate(across(c(mean, sd), forma))

# One-sided paired t-test
check_t <- check %>%
  t_test(
  mn ~ reason_type,
  paired = TRUE,
  alternative = "greater",
  ref.group = "sufficient") %>%
  mutate(p = formp(p), statistic = forma(statistic))

# Cohens dz
check_dz <- check %>%
  cohens_d(mn ~ reason_type, paired = TRUE, ref.group = "sufficient") %>%
  pull(effsize) %>%
  forma()

# Print t-test
check_report_plot <- paste0("t(", check_t$n1, ") = ", check_t$statistic,
  ",\n ", check_t$p, ",\n dz = ", check_dz)
check_report_text <- paste0("t(", check_t$n1, ") = ", check_t$statistic,
  ", ", check_t$p, ", d~z~ = ", check_dz)

# Plot results
check %>%
  {
    ggplot(., aes(x = mn, fill = reason_type, color = reason_type)) +
    labs(x = "Mean sufficiency rating", y = "Count",
       fill = "Reason type", color = "Reason type") +
    scale_x_continuous(limits = c(1, 5), oob = scales::oob_keep) +
    geom_histogram(position = "identity", alpha = .7, bins = 20) +
    scale_fill_manual(values = c("#849AB9", "#465263")) +
    scale_color_manual(values = c("#849AB9", "#465263")) +
    geom_label(aes(x = 4.3, y = 68, label = check_report_plot),
      fill = "white", show.legend = FALSE,
      label.padding = unit(0.5, "lines"), label.size = 0.5) +
    theme_cs_talk()
  } %T>%
  ggexport(width = 6, height = 5,
       filename = here::here("Analysis/check.pdf"))
```

We included a manipulation check for reason type. The sufficient reasons are supposed to sufficiently explain the behaviors, whereas the control reasons are supposed to insufficiently explain the behavior (or not explain the behavior at all). We thus asked participants at the end of the study to rate the sufficiency of each reason for the respective behavior and expected the sufficient reasons to score higher than the control reasons. To test this, we conducted a paired t-test. On average, participants gave significantly higher sufficiency ratings in the sufficient condition (M = `r check_desc$mean[2]`, SD = `r check_desc$sd[2]`) than in the control condition (M = `r check_desc$mean[1]`, SD = `r check_desc$sd[1]`), `r check_report_text`. We infer from this, that our manipulation of reason type worked.

### Preregistered analysis {.tabset}
```{r er analysis, message = FALSE}
# Prepare error rate data
er <- d_long %>%
  select(subject_id, probe_type, reason_type, is_correct, stimulus_set) %>%
  group_by(probe_type, reason_type, stimulus_set, subject_id) %>%
  summarize(.groups = "drop_last",
    errors = sum(is_correct == 0),
    n_trials = n()) %>%
  mutate(er = errors / n_trials, .keep = "unused") %>%
  # Find extreme outliers
  mutate(is_extreme = is_extreme(er)) %>%
  ungroup() %T>%
  # Count participants that have extreme outliers
  assign(x = "er_ex_n", envir = .GlobalEnv, value = n_distinct(
    select(filter(., is_extreme == TRUE), subject_id)))

# Create data frame where these rows are excluded
er_ex <- filter(er, is_extreme == FALSE)

# Descriptives
er_3f_desc <- er_ex %>%
  select(stimulus_set, reason_type, probe_type, er) %>%
  group_by(stimulus_set, reason_type, probe_type) %>%
  get_summary_stats(er, type = "mean_sd") %>%
  mutate(
    ci95_low = mean - 1.96 * sd / sqrt(n),
    ci95_upp = mean + 1.96 * sd / sqrt(n))

# Run ANOVA
er_3f_mod <- er_ex %>%
  anova_test(dv = er, wid = subject_id, effect.size = "pes",
    within = c(probe_type, reason_type), between = stimulus_set) %>%
  as_tibble() %>%
  rowwise() %>%
  mutate(F = forma(`F`, 2), p = formp(p), pes = forma(pes, 3, FALSE))

# Run post-hoc tests
er_3f_post <- er_ex %>%
  group_by(stimulus_set) %>%
  anova_test(dv = er, wid = subject_id, effect.size = "pes",
    within = c(probe_type, reason_type)) %>%
  as_tibble() %>%
  rowwise() %>%
  mutate(F = forma(`F`, 2), p = formp(p), pes = forma(pes, 3, FALSE))
```

We calculated false recognition rates by dividing the number of erroneous responses by six (the number of trials per condition).

#### Outlier detection
```{r er outlier, warning = FALSE, message = FALSE}
er_ex %>%
  {
    ggplot(., aes(reason_type, er, color = probe_type)) +
    geom_boxplot() +
    labs(x = "Reason type", y = "False recognition rate",
      color = "Probe type") +
    scale_color_manual(values = c("#849AB9", "#465263")) +
    geom_point(
      # Replace all non-outlier values by NA
      data = mutate(er, er = replace(er, is_extreme == FALSE, NA)),
      position = position_dodge(width = 0.75), shape = 4,
      size = 4, show.legend = FALSE) +
    theme_cs_talk()
  } %T>%
  ggexport(., width = 7, height = 5,
       filename = here("Analysis/er-outlier.pdf"))
```

We looked for extreme outliers in each cell of the design. We defined them as values above Q3 + 3 * IQR or below Q1 - 3 * IQR. We found and excluded `r er_ex_n` participants whose values were extreme outliers.

#### QQ-Plot
```{r er checking normality, message = FALSE}
er_ex %>%
  {
    ggplot(., aes(sample = er)) +
    labs(x = "Theoretical quantiles", y = "Data quantiles") +
    stat_qq(color = "#465263") +
    stat_qq_line(color = "#465263") +
    facet_grid(probe_type ~ reason_type, labeller = "label_value") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 7,
       filename = here("Analysis/er-qq.pdf"))
```

Upon visual inspection the distributions of error rates in the different conditions did not seem severely non-normal. We therefore applied no further transformations before conducting our analysis.

#### ANOVA
```{r er ANOVA}
knitr::kable(er_3f_mod, format = "markdown")
```

We conducted a three-way repeated-measures ANOVA to evaluate the effects of probe type, reason type, and stimulus set on the mean response latencies. The main effect for probe type was significant at the pre-registered alpha boundary of \alpha = .041, F(`r er_3f_mod$DFn[2]`, `r er_3f_mod$DFd[2]`) = `r er_3f_mod$F[2]`, `r formp(er_3f_mod$p[2])`, $\eta_{p}^{2}$ = `r er_3f_mod$pes[2]`. This indicates that the implied labels were spontaneously activated while participants read the statements and associated with the actors faces. There was no significant interaction between probe type and reason type, F(`r er_3f_mod$DFn[6]`, `r er_3f_mod$DFd[6]`) = `r er_3f_mod$F[6]`, `r formp(er_3f_mod$p[6])`, $\eta_{p}^{2}$ = `r er_3f_mod$pes[6]`. This suggests that no discounting occured when participants were presented with the sufficient compared to the control reasons.

However, there were unexpected effects involving the stimulus set. First, there was a small but significant main effect of stimulus set, F(`r er_3f_mod$DFn[1]`, `r er_3f_mod$DFd[1]`) = `r er_3f_mod$F[1]`, `r formp(er_3f_mod$p[1])`, $\eta_{p}^{2}$ = `r er_3f_mod$pes[1]`, indicating higher overall error rates in stimulus set B. Second, there was a significant interaction between stimulus set and probe type, F(`r er_3f_mod$DFn[4]`, `r er_3f_mod$DFd[4]`) = `r er_3f_mod$F[4]`, `r formp(er_3f_mod$p[4])`, $\eta_{p}^{2}$ = `r er_3f_mod$pes[4]`, with larger inference effects in stimulus set B. Lastly, there was a significant three-way interaction, F(`r er_3f_mod$DFn[7]`, `r er_3f_mod$DFd[7]`) = `r er_3f_mod$F[7]`, `r formp(er_3f_mod$p[7])`, $\eta_{p}^{2}$ = `r er_3f_mod$pes[7]`, indicating a larger discounting effect in stimulus set B.

#### Descriptives
```{r er descriptives}
knitr::kable(er_3f_desc, format = "markdown")
```

#### Lineplot
```{r er plotting results, message = FALSE}
er_3f_desc %>%
  {
    ggplot(., aes(reason_type, mean, color = probe_type, group = probe_type)) +
    facet_wrap(. ~ stimulus_set, labeller = "label_value") +
    labs(x = "Reason type", y = "Mean response latency in seconds",
      color = "Probe type") +
    scale_color_manual(values = c("#849AB9", "#465263")) +
    geom_line() +
    geom_point(size = 3) +
    geom_linerange(aes(ymin = ci95_low, ymax = ci95_upp)) +
    theme_cs_talk()
  } %T>%
  ggexport(width = 8, height = 5,
       filename = here::here("Analysis/er-lineplot.pdf"))
```

#### Post-hoc tests
```{r er post-hoc tests}
knitr::kable(er_3f_post, format = "markdown")
```

### Exploratory reaction time analysis
We conducted no exploratory reaction time analysis because we deemed the number of correct responses per cell to be too small to calculate reliable averages.

### Exploratory by-item analyses for the SII-effect {.tabset}
We conducted exploratory by-item analyses. We grouped the data by item_id, probe type, and reason type and calculated false recognition rates by dividing the number of erroneous responses by the number of trials. Then, for each item we calculated the SII-effect as a difference score (implied minus implied-other), with higher values indicating stronger inference effects. We only analyzed items containing a sufficient reason.

In this paradigm error rates are the primary response variable. We expect the error rates to less influenced by word length and currency than response latencies. We therefore calculated the difference score using the implied-other label presented with the same item (same item, different labels). We ran correlation analyses to investigate whether the SII-effect was stronger for items with reasons that were rated to be relatively sufficient. We also analyzed whether the SII-effect could be predicted using the ratings from study 2.

#### Correlation matrix
```{r by-item sii correlation, message = FALSE}
item_ratings_sii2 <- readRDS(here::here("Processed data/item-ratings-SII2.RDS"))
item_suff <- d_long %>%
  select(item_id, reason_type, sufficiency) %>%
  group_by(item_id, reason_type) %>%
  summarize(.groups = "drop", sufficiency = mean(sufficiency, na.rm = TRUE))
item <- d_long %>%
  select(item_id, behavior, probe_type, reason_type, is_correct) %>%
  group_by(item_id, behavior, probe_type, reason_type) %>%
  summarize(.groups = "drop",
    errors = sum(is_correct == 0),
    n_trials = n()) %>%
  mutate(er = errors / n_trials, .keep = "unused") %>%
  pivot_wider(names_from = probe_type, values_from = er) %>%
  mutate(sii_effect = implied - `implied other`, .keep = "unused") %>%
  left_join(item_suff) %>%
  left_join(item_ratings_sii2,
    by = c("item_id", "reason_type"),
    suffix = c("", "_sii2"))

c1 <- item %>%
  filter(reason_type == "sufficient") %>%
  select(sii_effect, sii_effect_sii2, identity, necessity,
  baserate, sufficiency, sufficiency_sii2) %>%
  cor_table()

knitr::kable(c1, format = "markdown")
```

There was a marginally significant correlation between the SII-effect of in this study and the SII-effect of study 2. This indicates that across two different experimental paradigms there is some degree of stability regarding the by-item effects. However, none of the ratings that predicted the SII-effect in study 2 also did so in study 3.

#### SII-effect from Study 2
```{r by-item sii sii-effect, message = FALSE}
filter(item, reason_type == "sufficient") %>%
  {
    ggplot(., aes(x = sii_effect_sii2, y = sii_effect)) +
    labs(x = "SII effect study 2", y = "SII effect study 3") +
    geom_smooth(method = "lm", formula = y ~ x, color = "#465263",
      size = 0.5, fill = "#849AB9") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.2, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263", max.overlaps = 15) +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-sii-sii2.pdf"))
```

#### Identity
```{r by-item sii identity, message = FALSE}
filter(item, reason_type == "sufficient") %>%
  {
    ggplot(., aes(x = identity, y = sii_effect)) +
    labs(x = "Mean identity rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-sii-identity.pdf"))
```

#### Necessity
```{r by-item sii necessity, message = FALSE}
filter(item, reason_type == "sufficient") %>%
  {
    ggplot(., aes(x = necessity, y = sii_effect)) +
    labs(x = "Mean necessity rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-sii-necessity.pdf"))
```

#### Baserate
```{r by-item sii baserate, message = FALSE}
filter(item, reason_type == "sufficient") %>%
  {
    ggplot(., aes(x = baserate, y = sii_effect)) +
    labs(x = "Mean baserate rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-sii-baserate.pdf"))
```

#### Sufficiency
```{r by-item sii sufficiency, message = FALSE}
filter(item, reason_type == "sufficient") %>%
  {
    ggplot(., aes(x = sufficiency, y = sii_effect)) +
    labs(x = "Mean sufficiency rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-sii-sufficiency.pdf"))
```

### Exploratory by-item analyses for the discounting effect {.tabset}
We conducted another set of exploratory by-item analyses with the discounting effect rather than the SII-effect as the dependent variable. For each item we calculated the discounting effect as a difference score (SII-effect for the sufficient reasons minus SII-effect for control reasons), with negative values indicating stronger discounting effects. For the sufficiency ratings we also calculated a difference score (sufficiency rating of the sufficient reasons minus sufficiency score of the control reasons).

We ran correlation analyses to investigate whether the SII-effect was stronger for items with reasons that were rated to be relatively sufficient. We also analyzed whether the SII-effect could be predicted using the ratings from study 2.

#### Correlation matrix
```{r by-item discounting correlation, message = FALSE}
item_disc <- item %>%
  pivot_wider(
    names_from = reason_type,
    values_from = c(sii_effect, sii_effect_sii2,
      sufficiency, sufficiency_sii2)) %>%
  mutate(.keep = "unused",
    sufficiency = sufficiency_sufficient - sufficiency_control,
    sufficiency_sii2 = sufficiency_sii2_sufficient - sufficiency_sii2_control,
    discounting = sii_effect_sufficient - sii_effect_control,
    discounting_sii2 = sii_effect_sii2_sufficient - sii_effect_sii2_control)

c2 <- item_disc %>%
  select(discounting, discounting_sii2, identity, necessity,
  baserate, sufficiency, sufficiency_sii2) %>%
  cor_table()

knitr::kable(c2, format = "markdown")
```

The discounting effect was not significantly correlated with any of the ratings. Discounting effects from study 1 and 2 were not correlated.

#### Discounting-effect from Study 2
```{r by-item discounting sii-effect, message = FALSE}
item_disc %>%
  {
    ggplot(., aes(x = discounting_sii2, y = discounting)) +
    labs(x = "Discounting effect study 2", y = "Discounting effect study 3") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.2, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263", max.overlaps = 15) +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-discounting-sii2.pdf"))
```

#### Identity
```{r by-item discounting identity, message = FALSE}
item_disc %>%
  {
    ggplot(., aes(x = identity, y = discounting)) +
    labs(x = "Mean identity rating", y = "Discounting effect") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-discounting-identity.pdf"))
```

#### Necessity
```{r by-item discounting necessity, message = FALSE}
item_disc %>%
  {
    ggplot(., aes(x = necessity, y = discounting)) +
    labs(x = "Mean necessity rating", y = "Discounting effect") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-discounting-necessity.pdf"))
```

#### Baserate
```{r by-item discounting baserate, message = FALSE}
item_disc %>%
  {
    ggplot(., aes(x = baserate, y = discounting)) +
    labs(x = "Mean baserate rating", y = "Discounting effect") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-discounting-baserate.pdf"))
```

#### Sufficiency
```{r by-item discounting sufficiency, message = FALSE}
item_disc %>%
  {
    ggplot(., aes(x = sufficiency, y = discounting)) +
    labs(x = "Mean sufficiency rating", y = "Discounting effect") +
    geom_abline(intercept = 0, slope = 0, color = "#465263") +
    geom_point(color = "#465263") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#465263") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-discounting-sufficiency.pdf"))
```

### Exploratory multilevel analysis {.tabset}
```{r moderation analyses, message = FALSE}
d_mlm <- d_long %>%
  select(
    subject_id, item_id, probe_type, reason_type, is_correct,
    sufficiency, gender, age, pol_orientation, pol_interest) %>%
  mutate(.keep = "unused",
    is_error = abs(1 - is_correct),
    gender = ifelse(gender %in% c("other", "not specified"), NA, gender),
    # Grand mean center level 2 predictors
    age = age - mean(age),
    pol_orientation = pol_orientation - mean(pol_orientation),
    pol_interest = pol_interest - mean(pol_interest),
    probe_type = relevel(probe_type, ref = "implied other")
  ) %>%
  group_by(subject_id) %>%
  mutate(
    sufficiency = sufficiency - mean(sufficiency),
    response = factor(ifelse(
      is_error == TRUE,
      "false recognitions",
      "correct rejections"))) %>%
  mutate(response = factor(response, levels = rev(levels(response)))) %>%
  ungroup()
```

We performed an exploratory multilevel analysis to investigate whether the ratings regarding the sufficiency of the reasons could explain the strength of the SII-effect.

```{r models}
mlm1 <- lme4::glmer(is_error ~ probe_type * reason_type +
  (1 | subject_id) + (1 | item_id),
  data = d_mlm,
  family = binomial(link = "logit"),
  control = glmerControl(optimizer = "bobyqa"))
mlm2 <- lme4::glmer(is_error ~ probe_type * sufficiency +
  (1 | subject_id) + (1 | item_id),
  data = d_mlm,
  family = binomial(link = "logit"),
  control = glmerControl(optimizer = "bobyqa"))
```

We ran two models. Each model had a random effect for participant and item. In Model I we predicted the binary response variable (correct rejection versus false recognition) by probe type and reason type. Only probe type was significant. In Model II we used sufficiency rather than reason type as a predictor. There was a significant effect of probe type, with the probability of a false recognition being higher for implied compared to implied-other probes. There was a marginally significant effect of sufficiency, with higher sufficiency ratings predicting a higher probability of a false recognition. Lastly, there was a significant interaction with an increase of sufficiency predicting a lower probability of a false recognition for implied probes.

#### Model I
```{r multilevel model 1, echo = FALSE}
sjPlot::tab_model(mlm1)
```

#### Model II
```{r multilevel model 2, echo = FALSE}
sjPlot::tab_model(mlm2)
```

#### Visualization sufficiency
```{r multilevel visualization sufficiency, message = FALSE, warning = FALSE, fig.width = 14}
d_mlm %>%
  {
  ggplot(., aes(x = sufficiency, y = is_error)) +
  facet_grid(~ probe_type, labeller = "label_value") +
  labs(
    y = "Probability of false recognition",
    x = "Sufficiency", color = "Response") +
  stat_smooth(
    method = "glm", size = 0.5, se = TRUE, level = 0.95,
    color = "#465263", fill = "#849AB9", alpha = 0.5,
    method.args = list(family = binomial)) +
  stat_slab(aes(y = is_error,
    side = ifelse(is_error == 1, "bottom", "top"),
    color = response), fill = "#E0E8ED", alpha = 1,
    slab_type = "pdf", scale = 0.25, size = 0.5) +
  scale_color_viridis_d(option = "mako", begin = 0.4, end = 0.85) +
  theme_cs_talk()
  } %T>%
  ggexport(width = 8, height = 5,
    filename = here::here("Analysis/mlm-sufficiency.pdf"))
```